{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Simple Wikipedia as retrieved data\n",
    "import pandas as pd\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def get_texts_splitted():\n",
    "    df = pd.read_parquet(\"hf://datasets/rahular/simple-wikipedia/data/train-00000-of-00001-090b52ccb189d47a.parquet\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    texts = list(df['text'])\n",
    "    text = ' '.join(texts)\n",
    "    texts_splitted = text_splitter.split_text(text)\n",
    "    return texts_splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34416/1366351712.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2', model_kwargs={'device':'cuda'})\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2', model_kwargs={'device':'cuda'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/chroma-core/chroma/issues/1049#issuecomment-1699859480\n",
    "\n",
    "def split_list(input_list, chunk_size):\n",
    "    res = []\n",
    "    for i in range(0, len(input_list), chunk_size):\n",
    "        res.append(input_list[i:i + chunk_size])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed():\n",
    "    texts_splitted = get_texts_splitted()\n",
    "    print(len(texts_splitted), texts_splitted[0])\n",
    "    texts_chunked = split_list(texts_splitted, 41000)\n",
    "    for ts in tqdm(texts_chunked):\n",
    "        db = Chroma.from_texts(ts, embedding_function, persist_directory='./embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "def get_chroma_retriever():\n",
    "    db = Chroma(persist_directory='./embed', embedding_function=embedding_function)\n",
    "    retriever = db.as_retriever(search_kwargs={'k':5})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "def get_bm25_retriever():\n",
    "    texts_splitted = get_texts_splitted()\n",
    "    texts_splitted = list(map(preprocess_text, tqdm(texts_splitted)))\n",
    "    retriever = BM25Retriever.from_texts(texts_splitted, k=8)\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34416/4023942215.py:4: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(persist_directory='./embed', embedding_function=embedding_function)\n"
     ]
    }
   ],
   "source": [
    "retriever = get_chroma_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "You are a Trivia expert with all the knowledge in the world. Below are the answers given by yourself. These are the guidelines followed:\n",
    "\n",
    "- Answer in a single word or a short phrase.\n",
    "- No extra information, explanations, or notes.\n",
    "- Do not include parenthetical statements.\n",
    "- Avoid any special characters or tags, such as \"less than\" and \"greater than\" symbols.\n",
    "- The answer must be directly relevant to the question.\n",
    "- Do not use phrases like \"None of the above\" unless it is the actual answer.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "Question: What is the chemical symbol for gold?\n",
    "Answer: Au\n",
    "\n",
    "Question: What is the capital of Australia?\n",
    "Answer: Canberra\n",
    "\n",
    "Question: Which artist is known for the painting \"The Starry Night\"?\n",
    "Answer: Vincent van Gogh\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "Answer: \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models.llms import LLM as BaseLLM\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "class LLM(BaseLLM):\n",
    "    model_name: str = None\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        extra = 'allow'\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name, \n",
    "            token=os.environ.get('token')\n",
    "        )\n",
    "        \n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"auto\",\n",
    "            token=os.environ.get('token')\n",
    "        )\n",
    "        \n",
    "        self.terminators = [\n",
    "            self.tokenizer.eos_token_id,\n",
    "            self.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "        \n",
    "        special_tokens_dict = {\n",
    "            \"pad_token\": \"<pad>\",\n",
    "            \"eos_token\": \"</s>\"\n",
    "        }\n",
    "        self.tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    def _call(self, prompt, stop=None, **kwargs):\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        \n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.model.device)\n",
    "        \n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=500,\n",
    "            eos_token_id=self.terminators,\n",
    "            pad_token_id=self.tokenizer.eos_token_id,\n",
    "            do_sample=False,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            top_k=None\n",
    "        )\n",
    "        \n",
    "        response = outputs[0][input_ids.shape[-1]:]\n",
    "        return self.tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        return {\"name\": \"LLM\", \"model_name\": self.model_name}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "llm = LLM('meta-llama/Meta-Llama-3-8B-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(output):\n",
    "    output = output['answer']\n",
    "    start_index = 0\n",
    "    end_index = output.find('\\n', start_index)\n",
    "    if end_index == -1:\n",
    "        end_index = len(output)\n",
    "    return output[start_index:end_index].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ross Bagdasarian'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = 'Who was the man behind Chipmunks?'\n",
    "input = {'input': preprocess_text(query)}\n",
    "\n",
    "response = retrieval_chain.invoke(input)\n",
    "clean_output(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use TriviaQA as Evaluation Dataset\n",
    "import json\n",
    "with open('./unfiltered-web-dev.json', 'r') as f:\n",
    "    data = json.load(f)['Data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_input(index):\n",
    "    example = data[index]\n",
    "    example_q = example['Question']\n",
    "    input = {'input': preprocess_text(example_q)}\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_answer(index):\n",
    "    example = data[index]\n",
    "    return example['Answer']['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_one(index):\n",
    "    response = retrieval_chain.invoke(index_to_input(index))\n",
    "    return index_to_answer(index), clean_output(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('David Seville', 'Ross Bagdasarian')\n",
      "('Scorpio', 'Scorpio')\n",
      "('Sunset Boulevard', 'Sunset Boulevard')\n",
      "('Campbell-Bannerman', 'Asquith')\n",
      "('Exile', 'Exile')\n",
      "('Cancer', 'Tuberculosis')\n",
      "('Octopussy', 'All Time High')\n",
      "('18 million', '25')\n",
      "('Utah', 'Utah')\n",
      "('Lauren Bacall', 'Lauren Bacall')\n",
      "('Nikkei', 'Nikkei')\n",
      "('Moonwalk', 'Moonwalk')\n",
      "('1930s', '1950s')\n",
      "('Hit the ball closer to the hole', 'Practice')\n",
      "('In 1912, in Stockholm', '1936')\n",
      "('Boxing rings were originally circular', 'Tradition')\n",
      "('$85,000', 'Ten')\n",
      "('Eighteen--two bears (one walking, one seated), a bison, camel, cougar, elephant, giraffe, gorilla, hippopotamus, hyena , kangaroo, lion, monkey, rhinoceros, seal, sheep, tier, and zebra', '8')\n",
      "('Kilimanjaro', 'Kilimanjaro')\n",
      "('Green', 'Green')\n"
     ]
    }
   ],
   "source": [
    "# Sample Pair of Ground Truth and Generated Output \n",
    "for i in range(20):\n",
    "    print(query_one(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_string(s):\n",
    "    s = preprocess_text(s)\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.translate import bleu\n",
    "from rouge import Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "def evaluation(answer, output):\n",
    "    answer = process_string(answer)\n",
    "    output = process_string(output)\n",
    "    em_score = 1 if answer == output else 0\n",
    "    bleu_score = bleu([output.split()], answer.split(), (1,))\n",
    "    rouge_score = rouge.get_scores(output, answer)\n",
    "    rouge_1_score = rouge_score[0]['rouge-1']['f']\n",
    "    rouge_l_score = rouge_score[0]['rouge-l']['f']\n",
    "    return torch.Tensor([em_score, bleu_score, rouge_1_score, rouge_l_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Example Evaluation\n",
    "for i in range(5):\n",
    "    print(evaluation(query_one(i)[0], query_one(i)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_of_tests=len(data)):\n",
    "    print('Create Input...')\n",
    "    inputs = list(map(index_to_input, range(num_of_tests)))\n",
    "    print('Running Inference...')\n",
    "    outputs = []\n",
    "    for input in tqdm(inputs):\n",
    "        outputs.append(retrieval_chain.invoke(input))\n",
    "    print('Cleaning Output...')\n",
    "    outputs = list(map(clean_output, outputs))\n",
    "    print('Evaluating...')\n",
    "    result = []\n",
    "    for i in range(num_of_tests):\n",
    "        ans = index_to_answer(i)\n",
    "        out = outputs[i]\n",
    "        result.append(evaluation(ans, out))\n",
    "    result = torch.stack(result)\n",
    "    result = torch.mean(result, dim=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Input...\n",
      "Running Inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [02:52<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Output...\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3820, 0.4569, 0.4781, 0.4729])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main(500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
